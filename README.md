# GovAI Knowledge Companion

A local, privacy-preserving Retrieval-Augmented Generation (RAG) system designed for government workflows. This project loads documents, builds vector embeddings, indexes them with ChromaDB, and answers user questions using an on-device Llama model via **Ollama**.

This README includes:

* ğŸš€ Project Overview
* ğŸ“‚ Recommended Folder Structure
* ğŸ› ï¸ Installation Steps
* â–¶ï¸ How to Run the Project
* ğŸ”§ Troubleshooting

---

## ğŸš€ Project Overview

The GovAI Knowledge Companion is a secure RAG application designed for government environments where cloud-based LLMs cannot be used. All processingâ€”including embeddings, retrieval, and generationâ€”runs **fully local** using:

* **Ollama** (local LLM serving)
* **Llama 3.1 (8B)** or any compatible model
* **ChromaDB** for vector storage
* **Streamlit** for the frontend UI

This allows government employees to upload or index policy documents and ask natural-language questions with chunk-level citations.

---

## ğŸ“‚ Recommended Folder Structure

Below is the clean, standardized structure for your repo. You can update your project to match this layout.

```
govai-knowledge-companion/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ embed.py
â”‚   â”œâ”€â”€ llm.py
â”‚   â””â”€â”€ rag.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_index.py
â”‚   â””â”€â”€ ingest_docs.py
â”‚
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ app.py            # Streamlit UI
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Original source documents
â”‚   â””â”€â”€ processed/        # Chunked text
â”‚
â”œâ”€â”€ chroma/               # Auto-created by ChromaDB
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

If you'd like, I can auto-generate all missing folders and files.

---

## ğŸ› ï¸ Installation Instructions

Follow these steps to install and run your project on Windows/macOS/Linux.

### 1ï¸âƒ£ Clone the repo

```
git clone https://github.com/<your-username>/govai-knowledge-companion.git
cd govai-knowledge-companion
```

### 2ï¸âƒ£ Create Virtual Environment

```
python -m venv venv
source venv/bin/activate      # macOS / Linux
venv\Scripts\activate         # Windows
```

### 3ï¸âƒ£ Install Dependencies

```
pip install -r requirements.txt
```

---

## ğŸ¦™ Install and Run Ollama

### Install Ollama

* Download from [https://ollama.com](https://ollama.com)

### Pull the Llama Model

```
ollama pull llama3.1:8b
```

### Start the Ollama Server

```
ollama serve
```

*(Must be running in a separate terminal window.)*

---

## ğŸ“š Build the Embedding Index

Before running the UI, you must build the vector database.

```
python -m scripts.build_index
```

This processes your documents â†’ chunks â†’ embeddings â†’ Chroma index.

---

## â–¶ï¸ Run the Streamlit App

```
streamlit run ui/app.py
```

The app launches here:

```
http://localhost:8501
```

---

## ğŸ”§ Troubleshooting

### **Ollama Port Already in Use**

```
netstat -ano | findstr 11434
```

Then kill the process:

```
taskkill /PID <PID> /F
```

### **ModuleNotFoundError: No module named 'backend'**

Make sure:

* `backend/` has `__init__.py`
* You are running from project root:

```
python -m ui.app
```

Or use Streamlit:

```
streamlit run ui/app.py
```

### **Ollama Client Errors**

Use the updated API:

```
response = ollama.generate(model=model, prompt=prompt)
```

### **Slow Generation**

Running locally means:

* Llama 8B uses CPU by default on most Windows machines
* For GPU acceleration, enable CUDA or use WSL2
